{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d687c844-3cd1-4f1e-b478-806364a5c7af",
      "metadata": {
        "id": "d687c844-3cd1-4f1e-b478-806364a5c7af"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int, verbose=False) -> None:\n",
        "    \"\"\"\n",
        "    vocab_size - the size of our vocabulary\n",
        "    d_model - the dimension of our embeddings and the input dimension for our model\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.embedding = ### YOUR CODE HERE\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.verbose:\n",
        "      print(f\"Embedding Vector (1st 5 elements): {self.embedding(x)[:5] * math.sqrt(self.d_model)}\")\n",
        "    return self.embedding(x) * math.sqrt(self.d_model) # scale embeddings by square root of d_model"
      ],
      "metadata": {
        "id": "RPoS_mQWuH7_"
      },
      "id": "RPoS_mQWuH7_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}